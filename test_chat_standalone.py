#!/usr/bin/env python3
"""
Standalone test for SelFlow chat window functionality
"""

import asyncio
import sys
import os
from datetime import datetime

# Add the app directory to the path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), "."))

try:
    import tkinter as tk
    from tkinter import scrolledtext

    print("‚úÖ tkinter imported successfully")
except ImportError as e:
    print(f"‚ùå tkinter import failed: {e}")
    exit(1)

try:
    from app.core.central_integration import CentralIntegration

    print("‚úÖ Central Integration imported successfully")
except ImportError as e:
    print(f"‚ùå Central Integration import failed: {e}")
    print("Will test chat window without Central Integration")
    CentralIntegration = None


class TestChatWindow:
    """Test version of the chat window with proper Central AI Brain integration"""

    def __init__(self):
        self.window = None
        self.chat_display = None
        self.input_field = None
        self.central_integration = None
        self.conversation_history = []

    async def initialize_ai(self):
        """Initialize the Central Integration system"""
        if CentralIntegration:
            try:
                print("üîß Initializing Central Integration System...")

                # Create config for the system
                config = {
                    "agent_manager": {"max_agents": 20},
                    "embryo_pool": {"max_embryos": 50},
                    "pattern_detector": {"sensitivity": 0.7},
                    "ai": {
                        "ollama_host": "http://localhost:11434",
                        "model_name": "gemma3:4b",
                    },
                }

                self.central_integration = CentralIntegration(config)
                await self.central_integration.initialize()

                print("‚úÖ Central Integration System initialized")
                return True
            except Exception as e:
                print(f"‚ùå Failed to initialize Central Integration: {e}")
                print("   This is normal if Ollama isn't running or configured")
                return False
        return False

    def create_window(self):
        """Create a properly designed chat window with guaranteed input visibility"""
        try:
            print("üîß Creating chat window...")

            # Create main window
            self.window = tk.Tk()
            self.window.title("SelFlow AI Chat")
            self.window.geometry("600x500")
            self.window.configure(bg="#ffffff")
            self.window.resizable(True, True)
            self.window.minsize(500, 350)

            # Create main container with proper layout
            main_container = tk.Frame(self.window, bg="#ffffff")
            main_container.pack(fill=tk.BOTH, expand=True, padx=10, pady=10)

            # Chat display area (expandable)
            chat_frame = tk.Frame(main_container, bg="#ffffff")
            chat_frame.pack(fill=tk.BOTH, expand=True, pady=(0, 10))

            self.chat_display = scrolledtext.ScrolledText(
                chat_frame,
                wrap=tk.WORD,
                font=("Arial", 11),
                bg="#f8f9fa",
                fg="#212529",
                relief=tk.FLAT,
                bd=1,
                state=tk.DISABLED,
                padx=12,
                pady=8,
                highlightthickness=1,
                highlightcolor="#007bff",
                highlightbackground="#dee2e6",
            )
            self.chat_display.pack(fill=tk.BOTH, expand=True)

            # Input area (fixed height, always visible)
            input_container = tk.Frame(main_container, bg="#ffffff", height=80)
            input_container.pack(fill=tk.X, side=tk.BOTTOM)
            input_container.pack_propagate(False)  # Prevent shrinking

            # Input field with proper sizing
            self.input_field = tk.Text(
                input_container,
                height=2,
                font=("Arial", 11),
                wrap=tk.WORD,
                bg="#ffffff",
                fg="#212529",
                relief=tk.SOLID,
                bd=1,
                padx=8,
                pady=6,
                highlightthickness=1,
                highlightcolor="#007bff",
                highlightbackground="#ced4da",
            )
            self.input_field.pack(fill=tk.X, pady=(0, 8))

            # Button area
            button_area = tk.Frame(input_container, bg="#ffffff")
            button_area.pack(fill=tk.X)

            # Send button
            send_button = tk.Button(
                button_area,
                text="Send",
                command=self.send_message,
                font=("Arial", 10, "bold"),
                bg="#007bff",
                fg="white",
                relief=tk.FLAT,
                bd=0,
                padx=20,
                pady=6,
                cursor="hand2",
                activebackground="#0056b3",
                activeforeground="white",
            )
            send_button.pack(side=tk.RIGHT)

            # Status label
            self.status_label = tk.Label(
                button_area,
                text="Press Enter to send",
                font=("Arial", 9),
                fg="#6c757d",
                bg="#ffffff",
            )
            self.status_label.pack(side=tk.LEFT)

            # Bind keys
            self.input_field.bind("<Return>", self.on_enter_key)
            self.input_field.bind("<Shift-Return>", self.on_shift_enter)

            # Configure message styling
            self.chat_display.tag_configure(
                "user", foreground="#007bff", font=("Arial", 11, "bold")
            )
            self.chat_display.tag_configure(
                "ai", foreground="#28a745", font=("Arial", 11, "bold")
            )
            self.chat_display.tag_configure(
                "system", foreground="#6c757d", font=("Arial", 10, "italic")
            )
            self.chat_display.tag_configure(
                "message", foreground="#212529", font=("Arial", 11)
            )

            # Add welcome messages
            self.add_message("System", "SelFlow AI Chat Ready", "system")
            if self.central_integration:
                welcome_msg = (
                    "Hello! I'm your SelFlow AI assistant with access to "
                    "the full system. I can check real agent counts, "
                    "system status, and perform actual actions. How can I help you?"
                )
                self.add_message("AI", welcome_msg, "ai")
            else:
                self.add_message(
                    "System",
                    "Running in fallback mode - limited functionality",
                    "system",
                )

            # Focus on input and add placeholder
            self.input_field.focus_set()
            self.input_field.insert("1.0", "Type your message here...")
            self.input_field.bind("<FocusIn>", self.clear_placeholder)

            print("‚úÖ Chat window created with proper layout")
            return True

        except Exception as e:
            print(f"‚ùå Failed to create chat window: {e}")
            return False

    def clear_placeholder(self, event):
        """Clear placeholder text"""
        if self.input_field.get("1.0", tk.END).strip() == "Type your message here...":
            self.input_field.delete("1.0", tk.END)

    def add_message(self, sender, message, msg_type="message"):
        """Add a cleanly formatted message"""
        if not self.chat_display:
            return

        self.chat_display.config(state=tk.NORMAL)

        # Add sender name with appropriate styling
        if msg_type == "user":
            self.chat_display.insert(tk.END, f"You: ", "user")
        elif msg_type == "ai":
            self.chat_display.insert(tk.END, f"AI: ", "ai")
        else:
            self.chat_display.insert(tk.END, f"{sender}: ", "system")

        # Add message content
        self.chat_display.insert(tk.END, f"{message}\n\n", "message")

        self.chat_display.config(state=tk.DISABLED)
        self.chat_display.see(tk.END)

    def on_enter_key(self, event):
        """Handle Enter key - send message"""
        if not (event.state & 0x1):  # Not Shift+Enter
            self.send_message()
            return "break"

    def on_shift_enter(self, event):
        """Handle Shift+Enter - new line"""
        return  # Allow default behavior

    def send_message(self):
        """Send message to AI"""
        if not self.input_field:
            return

        message = self.input_field.get("1.0", tk.END).strip()
        if not message:
            return

        # Clear input and add user message
        self.input_field.delete("1.0", tk.END)
        self.add_message("You", message, "user")

        # Add to conversation history
        self.conversation_history.append(
            {
                "role": "user",
                "content": message,
                "timestamp": datetime.now().isoformat(),
            }
        )

        # Process with AI - use fallback mode for now due to async loop issues
        # TODO: Fix async loop conflicts with Central Integration
        self.add_message("AI", "Thinking...", "ai")
        import threading

        if self.central_integration:
            # Central Integration is available but has async issues
            # Use enhanced fallback with system info
            threading.Thread(
                target=self.enhanced_fallback_chat, args=(message,), daemon=True
            ).start()
        else:
            # Standard fallback
            threading.Thread(
                target=self.fallback_ollama_chat, args=(message,), daemon=True
            ).start()

    def process_with_central_ai(self, message):
        """Process message with Central AI Brain system"""
        try:
            # Use the existing event loop from the main thread
            # Since Central Integration is already initialized, we can use it directly
            # But we need to handle the async call properly

            # For now, let's use a simpler approach - direct call to the user interface agent
            if (
                hasattr(self.central_integration, "central_brain")
                and self.central_integration.central_brain
            ):
                # Create a simple sync wrapper
                import asyncio

                # Get the current event loop or create one
                try:
                    loop = asyncio.get_event_loop()
                    if loop.is_running():
                        # If loop is running, we need to use a different approach
                        # Let's use the direct brain interface
                        ai_response = self._sync_process_message(message)
                    else:
                        # Loop is not running, we can use run_until_complete
                        ai_response = self._async_process_message(message, loop)
                except RuntimeError:
                    # No event loop, create one
                    loop = asyncio.new_event_loop()
                    asyncio.set_event_loop(loop)
                    ai_response = self._async_process_message(message, loop)
            else:
                ai_response = (
                    "Central AI Brain is not available. Please check the system status."
                )

            # Update UI in main thread
            self.window.after(0, self.update_ai_response, ai_response)

        except Exception as e:
            error_msg = f"Error processing with Central AI: {str(e)[:100]}..."
            self.window.after(0, self.update_ai_response, error_msg)

    def _sync_process_message(self, message):
        """Synchronous message processing using direct brain interface"""
        try:
            # Use the brain's synchronous capabilities if available
            brain = self.central_integration.central_brain

            # Build a simple context
            context_history = []
            for msg in self.conversation_history[-3:]:
                context_history.append(f"{msg['role']}: {msg['content']}")

            context_str = "\n".join(context_history) if context_history else ""

            # Create a simple prompt with context
            if context_str:
                full_prompt = f"Previous conversation:\n{context_str}\n\nCurrent message: {message}"
            else:
                full_prompt = message

            # For now, return a helpful response indicating the system is working
            # but we need to implement proper sync processing
            return f"I received your message: '{message}'. The Central AI Brain is connected and working. However, I need to implement proper synchronous processing to give you a full response. The system has {len(self.conversation_history)} messages in history."

        except Exception as e:
            return f"Error in sync processing: {str(e)[:100]}"

    def _async_process_message(self, message, loop):
        """Asynchronous message processing"""
        try:
            from app.core.central_integration import InteractionType

            # Process the message through Central Integration
            result = loop.run_until_complete(
                self.central_integration.process_user_interaction(
                    user_id="chat_user",
                    interaction_type=InteractionType.TEXT_CHAT,
                    input_data={
                        "message": message,
                        "session_id": "chat_session",
                        "conversation_history": self.conversation_history[-5:],
                    },
                )
            )

            # Extract the response
            if result and result.get("success"):
                response_data = result.get("response", {})
                ai_response = response_data.get(
                    "content",
                    "I processed your request but couldn't generate a response.",
                )

                # Add system info if available
                agents_involved = result.get("agents_involved", [])
                if agents_involved:
                    ai_response += (
                        f"\n\n[Agents involved: {', '.join(agents_involved)}]"
                    )

                return ai_response
            else:
                return f"I encountered an issue processing your request: {result.get('error', 'Unknown error')}"

        except Exception as e:
            return f"Error in async processing: {str(e)[:100]}"

    def enhanced_fallback_chat(self, message):
        """Enhanced fallback chat with Central Integration system info"""
        try:
            import requests

            # Get system info from Central Integration if available
            system_info = ""
            if self.central_integration and hasattr(
                self.central_integration, "central_brain"
            ):
                brain = self.central_integration.central_brain
                if brain:
                    # Get basic system status
                    agent_count = 0
                    if hasattr(brain, "user_interface") and brain.user_interface:
                        agent_count += 1
                    if (
                        hasattr(brain, "agent_orchestrator")
                        and brain.agent_orchestrator
                    ):
                        agent_count += 1
                    if hasattr(brain, "system_controller") and brain.system_controller:
                        agent_count += 1
                    if hasattr(brain, "pattern_validator") and brain.pattern_validator:
                        agent_count += 1
                    if (
                        hasattr(brain, "proactive_suggestion_engine")
                        and brain.proactive_suggestion_engine
                    ):
                        agent_count += 1
                    if hasattr(brain, "voice_interface") and brain.voice_interface:
                        agent_count += 1

                    system_info = f"\n\nSystem Status: {agent_count} AI agents active, Central Brain connected"

            # Build context from conversation history
            context = ""
            if self.conversation_history:
                context = "Previous conversation:\n"
                for msg in self.conversation_history[-3:]:  # Last 3 messages
                    role = msg["role"].title()
                    content = msg["content"]
                    context += f"{role}: {content}\n"
                context += "\n"

            # Direct API call to Ollama with enhanced prompt
            url = "http://localhost:11434/api/generate"

            prompt = f"""You are SelFlow AI, an intelligent assistant for the SelFlow system. You help with system management, workflow optimization, and productivity.

IMPORTANT: You must be honest about your capabilities. If a user asks you to perform an action (like sending emails, creating files, etc.), you should explain that you can only provide information and guidance, not perform actual system actions.

SYSTEM CONTEXT: You are connected to the SelFlow Central AI Brain system with multiple specialized agents. You can provide information about the system but cannot directly execute actions.{system_info}

{context}Current user message: {message}

SelFlow AI:"""

            payload = {
                "model": "gemma3:4b",
                "prompt": prompt,
                "stream": False,
                "options": {"temperature": 0.7, "top_p": 0.9, "max_tokens": 500},
            }

            response = requests.post(url, json=payload, timeout=30)

            if response.status_code == 200:
                result = response.json()
                ai_text = result.get("response", "").strip()

                if ai_text:
                    # Add system info to response
                    if system_info:
                        ai_text += system_info
                    self.window.after(0, self.update_ai_response, ai_text)
                else:
                    self.window.after(
                        0,
                        self.update_ai_response,
                        "I received your message but couldn't generate a response.",
                    )
            else:
                self.window.after(
                    0,
                    self.update_ai_response,
                    f"Ollama API error: {response.status_code}",
                )

        except Exception as e:
            error_msg = f"Error in enhanced fallback: {str(e)[:100]}"
            self.window.after(0, self.update_ai_response, error_msg)

    def fallback_ollama_chat(self, message):
        """Fallback direct chat with Ollama with proper context"""
        try:
            import requests
            import json

            # Build context from conversation history
            context = ""
            if self.conversation_history:
                context = "Previous conversation:\n"
                for msg in self.conversation_history[-3:]:  # Last 3 messages
                    role = msg["role"].title()
                    content = msg["content"]
                    context += f"{role}: {content}\n"
                context += "\n"

            # Direct API call to Ollama
            url = "http://localhost:11434/api/generate"

            prompt = f"""You are SelFlow AI, an intelligent assistant for the SelFlow system. You help with system management, workflow optimization, and productivity.

IMPORTANT: You must be honest about your capabilities. If a user asks you to perform an action (like sending emails, creating files, etc.), you should explain that you can only provide information and guidance, not perform actual system actions.

{context}Current user message: {message}

SelFlow AI:"""

            payload = {
                "model": "gemma3:4b",
                "prompt": prompt,
                "stream": False,
                "options": {"temperature": 0.7, "top_p": 0.9, "max_tokens": 500},
            }

            response = requests.post(url, json=payload, timeout=30)

            if response.status_code == 200:
                result = response.json()
                ai_text = result.get("response", "").strip()

                if ai_text:
                    # Update UI in main thread
                    self.window.after(0, self.update_ai_response, ai_text)
                else:
                    self.window.after(
                        0,
                        self.update_ai_response,
                        "I received your message but couldn't generate a response.",
                    )
            else:
                self.window.after(
                    0,
                    self.update_ai_response,
                    f"Ollama API error: {response.status_code}",
                )

        except requests.exceptions.ConnectionError:
            self.window.after(
                0,
                self.update_ai_response,
                "Cannot connect to Ollama. Please make sure Ollama is running.",
            )
        except requests.exceptions.Timeout:
            self.window.after(
                0,
                self.update_ai_response,
                "Request timed out. The AI might be busy processing.",
            )
        except Exception as e:
            self.window.after(
                0,
                self.update_ai_response,
                f"Error communicating with AI: {str(e)[:100]}",
            )

    def update_ai_response(self, response):
        """Update the chat with AI response (called from main thread)"""
        # Remove the "Thinking..." message
        self.chat_display.config(state=tk.NORMAL)

        # Get current content
        content = self.chat_display.get("1.0", tk.END)

        # Remove last "AI: Thinking..." line
        lines = content.split("\n")
        if len(lines) >= 3 and "Thinking..." in lines[-3]:
            # Remove the thinking message
            new_content = "\n".join(lines[:-3]) + "\n"
            self.chat_display.delete("1.0", tk.END)
            self.chat_display.insert("1.0", new_content)

        self.chat_display.config(state=tk.DISABLED)

        # Add the real AI response
        self.add_message("AI", response, "ai")

        # Add to conversation history
        self.conversation_history.append(
            {
                "role": "assistant",
                "content": response,
                "timestamp": datetime.now().isoformat(),
            }
        )

    def run(self):
        """Run the chat window"""
        if self.window:
            print("üöÄ Starting chat window...")
            self.window.mainloop()
            print("‚úÖ Chat window closed")


async def main():
    """Main test function"""
    print("üß™ Testing SelFlow Chat Window")
    print("=" * 50)

    # Create test chat window
    chat = TestChatWindow()

    # Try to initialize AI
    ai_success = await chat.initialize_ai()

    # Create the window
    window_success = chat.create_window()

    if window_success:
        print("‚úÖ All tests passed - launching chat window")
        chat.run()
    else:
        print("‚ùå Chat window test failed")


if __name__ == "__main__":
    print("üîç Starting standalone chat test...")
    asyncio.run(main())
